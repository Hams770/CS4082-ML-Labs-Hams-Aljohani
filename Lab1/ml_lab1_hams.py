# -*- coding: utf-8 -*-
"""ML_Lab1_Hams.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xI-DXi1HZ2AYgNqOpKCWct65MDNI9H7H

# üß† Machine Learning - Lab 1 Hams Aljohani
## Foundations: ML Lifecycle, Environment Setup & Data Preprocessing

---

**Course:** CS4082 Machine Learning  
**Institution:** Effat University, Computer Science Department  
**Instructor:** Dr. Naila Marir

---

### üìã Lab Objectives

By the end of this lab, you will be able to:

1. **Understand** the complete Machine Learning project lifecycle
2. **Set up** your development environment using Google Colab and/or VSCode
3. **Apply** NumPy and Pandas operations for data manipulation
4. **Perform** essential data preprocessing techniques using Pandas

---

# Part 1: Machine Learning Project Lifecycle üîÑ

Every successful ML project follows a structured lifecycle. Understanding this lifecycle is crucial before diving into implementation.

## 1.1 The Seven Stages of ML Project Lifecycle

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     MACHINE LEARNING PROJECT LIFECYCLE                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ   ‚îÇ 1. Problem   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 2. Data      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 3. Data      ‚îÇ                 ‚îÇ
‚îÇ   ‚îÇ Definition   ‚îÇ    ‚îÇ Collection   ‚îÇ    ‚îÇ Preparation  ‚îÇ                 ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ          ‚îÇ                                       ‚îÇ                          ‚îÇ
‚îÇ          ‚îÇ                                       ‚ñº                          ‚îÇ
‚îÇ          ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ          ‚îÇ            ‚îÇ 5. Model     ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ 4. Feature   ‚îÇ                 ‚îÇ
‚îÇ          ‚îÇ            ‚îÇ Training     ‚îÇ    ‚îÇ Engineering  ‚îÇ                 ‚îÇ
‚îÇ          ‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ          ‚îÇ                   ‚îÇ                                              ‚îÇ
‚îÇ          ‚îÇ                   ‚ñº                                              ‚îÇ
‚îÇ          ‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 6. Model     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ 7. Model     ‚îÇ                 ‚îÇ
‚îÇ         (iterate)     ‚îÇ Evaluation   ‚îÇ    ‚îÇ Deployment   ‚îÇ                 ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                              ‚îÇ                   ‚îÇ                          ‚îÇ
‚îÇ                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ                                 (monitoring & feedback)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## 1.2 Detailed Breakdown of Each Stage

### Stage 1: Problem Definition üéØ

**Key Questions to Answer:**
- What business/research problem are we trying to solve?
- Is ML the right approach for this problem?
- What type of ML task is this? (Classification, Regression, Clustering, etc.)
- What does success look like? (Define metrics)

**Example:**
- **Problem:** Predict student performance to enable early intervention
- **ML Task Type:** Regression (predicting GPA) or Classification (Pass/Fail)
- **Success Metric:** Accuracy > 85%, or RMSE < 0.5

---

### Stage 2: Data Collection üìä

**Key Activities:**
- Identify data sources (databases, APIs, files, web scraping)
- Collect relevant data
- Ensure data quality and quantity
- Consider ethical and legal aspects (privacy, consent)

**Common Data Sources:**
- Kaggle datasets
- UCI Machine Learning Repository
- Government open data portals
- Company databases
- Web APIs

---

### Stage 3: Data Preparation (Preprocessing) üßπ

**This is where we spend 60-80% of our time!**

**Key Activities:**
- Data cleaning (handling missing values, duplicates)
- Data transformation (normalization, encoding)
- Data integration (merging multiple sources)
- Exploratory Data Analysis (EDA)

---

### Stage 4: Feature Engineering üîß

**Key Activities:**
- Feature selection (choosing relevant features)
- Feature creation (deriving new features)
- Feature transformation (log, polynomial)
- Dimensionality reduction (PCA, etc.)

---

### Stage 5: Model Training üèãÔ∏è

**Key Activities:**
- Select appropriate algorithms
- Split data (train/validation/test)
- Train models
- Tune hyperparameters

---

### Stage 6: Model Evaluation üìà

**Key Activities:**
- Evaluate using appropriate metrics
- Compare different models
- Analyze errors and failures
- Validate on unseen data

---

### Stage 7: Model Deployment üöÄ

**Key Activities:**
- Deploy model to production
- Monitor performance
- Update and retrain as needed
- Document and maintain

## üí° Exercise 1.1: Problem Definition Practice

For each scenario below, identify:
1. The ML task type
2. Potential input features
3. The target variable
4. A suitable evaluation metric

**Scenarios:**

**A)** A hospital wants to predict whether a patient will be readmitted within 30 days.

**B)** An e-commerce company wants to group customers based on their shopping behavior.

**C)** A real estate company wants to estimate house prices.

**Write your answers in the cell below:**

**Your Answers:**

**A) Hospital Readmission:**
- ML Task Type: Binary Classification
- Input Features: Patient demographics (age, gender, insurance), medical history (diagnoses, medications, procedures), previous hospital visits, lab results.
- Target Variable:Readmission within 30 days (e.g., 0 = No, 1 = Yes)
- Evaluation Metric:Recall, F1-Score, AUC-ROC (to prioritize correctly identifying readmissions)

**B) Customer Grouping:**
- ML Task Type:Clustering
- Input Features:Purchase history (frequency, monetary value, types of products), browsing behavior, demographics, response to promotions.
- Target Variable: None (clustering is unsupervised, aiming to discover patterns)
- Evaluation Metric:Silhouette Score, Davies-Bouldin Index, or domain expertise for interpretability.

**C) House Prices:**
- ML Task Type:Regression
- Input Features:Number of bedrooms/bathrooms, square footage, lot size, location (neighborhood, school district), age of property, presence of amenities (garage, pool).
- Target Variable:House Price (continuous numerical value)
- Evaluation Metric:Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), R-squared.

---

# Part 2: Environment Setup üíª

## 2.1 Google Colab

Google Colab is a free, cloud-based Jupyter notebook environment that requires no setup.

### Advantages:
- ‚úÖ No installation required
- ‚úÖ Free GPU/TPU access
- ‚úÖ Pre-installed ML libraries
- ‚úÖ Easy sharing and collaboration
- ‚úÖ Google Drive integration

### How to Access:
1. Go to [colab.research.google.com](https://colab.research.google.com)
2. Sign in with your Google account
3. Create a new notebook or upload an existing one

### Useful Colab Shortcuts:
| Shortcut | Action |
|----------|--------|
| `Ctrl + Enter` | Run current cell |
| `Shift + Enter` | Run cell and move to next |
| `Ctrl + M B` | Insert cell below |
| `Ctrl + M A` | Insert cell above |
| `Ctrl + M D` | Delete cell |
| `Ctrl + M M` | Convert to markdown |
| `Ctrl + M Y` | Convert to code |
"""

# Check if running in Colab
import sys

IN_COLAB = 'google.colab' in sys.modules
print(f"Running in Google Colab: {IN_COLAB}")

if IN_COLAB:
    print("üéâ Welcome to Google Colab!")
else:
    print("üìç You're running this locally (VSCode or Jupyter)")

# Mounting Google Drive (Colab only)
# This allows you to save and access files from your Drive

if IN_COLAB:
    from google.colab import drive
    drive.mount('/content/drive')
    print("‚úÖ Google Drive mounted successfully!")

"""## 2.2 VSCode Setup

Visual Studio Code is a powerful, customizable code editor.

### Installation Steps:

1. **Install VSCode:** Download from [code.visualstudio.com](https://code.visualstudio.com)

2. **Install Python:** Download from [python.org](https://python.org) (version 3.8+)

3. **Install Required Extensions:**
   - Python (Microsoft)
   - Jupyter (Microsoft)
   - Pylance (Microsoft)

4. **Create a Virtual Environment:**
   ```bash
   # In terminal
   python -m venv ml_env
   
   # Activate (Windows)
   ml_env\Scripts\activate
   
   # Activate (Mac/Linux)
   source ml_env/bin/activate
   ```

5. **Install Required Libraries:**
   ```bash
   pip install numpy pandas matplotlib seaborn scikit-learn jupyter
   ```
"""

# Check installed packages and versions
import numpy as np
import pandas as pd
import matplotlib
import sys

print("="*50)
print("üîç ENVIRONMENT CHECK")
print("="*50)
print(f"Python Version: {sys.version}")
print(f"NumPy Version: {np.__version__}")
print(f"Pandas Version: {pd.__version__}")
print(f"Matplotlib Version: {matplotlib.__version__}")
print("="*50)
print("‚úÖ All essential libraries are installed!")

"""---

# Part 3: NumPy & Pandas Revision üìö

## 3.1 NumPy Essentials

NumPy (Numerical Python) is the foundation of scientific computing in Python.
"""

import numpy as np

# ============================================
# CREATING ARRAYS
# ============================================

# From Python lists
arr1 = np.array([1, 2, 3, 4, 5])
print("1D Array:", arr1)

# 2D Array (Matrix)
arr2 = np.array([[1, 2, 3],
                 [4, 5, 6]])
print("\n2D Array:")
print(arr2)

# Special arrays
zeros = np.zeros((3, 3))          # 3x3 array of zeros
ones = np.ones((2, 4))            # 2x4 array of ones
identity = np.eye(3)              # 3x3 identity matrix
random_arr = np.random.rand(3, 3) # 3x3 random values [0,1)

print("\nZeros Matrix:")
print(zeros)

print("\nIdentity Matrix:")
print(identity)

# ============================================
# ARRAY PROPERTIES
# ============================================

arr = np.array([[1, 2, 3, 4],
                [5, 6, 7, 8],
                [9, 10, 11, 12]])

print("Array:")
print(arr)
print(f"\nShape: {arr.shape}")      # (rows, columns)
print(f"Dimensions: {arr.ndim}")    # Number of dimensions
print(f"Size: {arr.size}")          # Total elements
print(f"Data Type: {arr.dtype}")    # Data type of elements

# ============================================
# INDEXING AND SLICING
# ============================================

arr = np.array([[1, 2, 3, 4],
                [5, 6, 7, 8],
                [9, 10, 11, 12]])

print("Original Array:")
print(arr)

# Accessing elements
print(f"\nElement at [0,0]: {arr[0, 0]}")    # First element
print(f"Element at [1,2]: {arr[1, 2]}")      # Row 1, Column 2

# Slicing
print(f"\nFirst row: {arr[0, :]}")           # All columns of row 0
print(f"First column: {arr[:, 0]}")          # All rows of column 0
print(f"\nSubarray [0:2, 1:3]:")
print(arr[0:2, 1:3])                          # Rows 0-1, Columns 1-2

# ============================================
# ARRAY OPERATIONS
# ============================================

a = np.array([1, 2, 3, 4])
b = np.array([5, 6, 7, 8])

print("a =", a)
print("b =", b)

# Element-wise operations
print(f"\na + b = {a + b}")
print(f"a * b = {a * b}")
print(f"a ** 2 = {a ** 2}")

# Statistical operations
print(f"\nSum of a: {np.sum(a)}")
print(f"Mean of a: {np.mean(a)}")
print(f"Std of a: {np.std(a):.2f}")
print(f"Max of a: {np.max(a)}")
print(f"Min of a: {np.min(a)}")

# ============================================
# RESHAPING ARRAYS
# ============================================

arr = np.arange(1, 13)  # [1, 2, 3, ..., 12]
print("Original (1D):", arr)

# Reshape to different dimensions
reshaped_3x4 = arr.reshape(3, 4)
print("\nReshaped to 3x4:")
print(reshaped_3x4)

reshaped_4x3 = arr.reshape(4, 3)
print("\nReshaped to 4x3:")
print(reshaped_4x3)

# Flatten back to 1D
flattened = reshaped_3x4.flatten()
print("\nFlattened:", flattened)

"""## üí° Exercise 3.1: NumPy Practice

Complete the following tasks:
"""

# Task 1: Create a 5x5 array with values from 1 to 25
# Your code here:



# Task 2: Extract the diagonal elements of the array
# Hint: use np.diag()
# Your code here:



# Task 3: Calculate the sum of each row
# Hint: use np.sum() with axis parameter
# Your code here:



# Task 4: Normalize the array (subtract mean, divide by std)
# Your code here:

import numpy as np

# Task 1: Create a 5x5 array with values from 1 to 25
# Your code here:
array_5x5 = np.arange(1, 26).reshape(5, 5)
print("Task 1: 5x5 array")
print(array_5x5)


# Task 2: Extract the diagonal elements of the array
# Hint: use np.diag()
# Your code here:
diagonal_elements = np.diag(array_5x5)
print("Task 2: Diagonal elements")
print(diagonal_elements)


# Task 3: Calculate the sum of each row
# Hint: use np.sum() with axis parameter
# Your code here:
row_sums = np.sum(array_5x5, axis=1)
print("Task 3: Sum of each row")
print(row_sums)


# Task 4: Normalize the array (subtract mean, divide by std)
# Your code here:
mean = np.mean(array_5x5)
std = np.std(array_5x5)
normalized_array = (array_5x5 - mean) / std
print("Task 4: Normalized array")
print(normalized_array)

"""## 3.2 Pandas Essentials

Pandas provides high-performance, easy-to-use data structures for data analysis.
"""

import pandas as pd

# ============================================
# SERIES - 1D labeled array
# ============================================

# Creating a Series
grades = pd.Series([85, 90, 78, 92, 88],
                   index=['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'])

print("Student Grades (Series):")
print(grades)

# Accessing elements
print(f"\nAlice's grade: {grades['Alice']}")
print(f"Mean grade: {grades.mean():.2f}")

# ============================================
# DATAFRAME - 2D labeled data structure
# ============================================

# Creating a DataFrame from dictionary
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'Age': [22, 25, 23, 24, 22],
    'Major': ['CS', 'Math', 'CS', 'Physics', 'CS'],
    'GPA': [3.8, 3.5, 3.9, 3.7, 3.6]
}

df = pd.DataFrame(data)
print("Student DataFrame:")
print(df)

# ============================================
# EXPLORING DATA
# ============================================

print("Basic Information:")
print(f"Shape: {df.shape}")  # (rows, columns)
print(f"\nColumn names: {list(df.columns)}")
print(f"\nData types:\n{df.dtypes}")

# Viewing data
print("First 3 rows (head):")
print(df.head(3))

print("\nLast 2 rows (tail):")
print(df.tail(2))

print("\nStatistical summary:")
print(df.describe())

# ============================================
# SELECTING DATA
# ============================================

# Selecting a single column (returns Series)
print("Names column:")
print(df['Name'])

# Selecting multiple columns (returns DataFrame)
print("\nName and GPA columns:")
print(df[['Name', 'GPA']])

# ============================================
# INDEXING WITH .loc AND .iloc
# ============================================

# .loc - label-based indexing
print("Using .loc (label-based):")
print(df.loc[0])              # Row with index 0
print()
print(df.loc[0:2, 'Name':'Major'])  # Rows 0-2, columns Name to Major

# .iloc - integer-based indexing
print("\nUsing .iloc (integer-based):")
print(df.iloc[0])             # First row
print()
print(df.iloc[0:2, 0:2])      # First 2 rows, first 2 columns

# ============================================
# FILTERING DATA
# ============================================

# Boolean filtering
print("Students with GPA > 3.6:")
print(df[df['GPA'] > 3.6])

print("\nCS majors:")
print(df[df['Major'] == 'CS'])

# Multiple conditions
print("\nCS majors with GPA >= 3.8:")
print(df[(df['Major'] == 'CS') & (df['GPA'] >= 3.8)])

# ============================================
# ADDING AND MODIFYING COLUMNS
# ============================================

# Adding a new column
df['Scholarship'] = df['GPA'] >= 3.7
print("With Scholarship column:")
print(df)

# Creating a column based on conditions
df['Grade_Letter'] = df['GPA'].apply(
    lambda x: 'A' if x >= 3.7 else ('B' if x >= 3.3 else 'C')
)
print("\nWith Grade Letter:")
print(df)

# ============================================
# GROUPING AND AGGREGATION
# ============================================

# Group by Major
print("Average GPA by Major:")
print(df.groupby('Major')['GPA'].mean())

print("\nCount by Major:")
print(df.groupby('Major').size())

print("\nMultiple aggregations:")
print(df.groupby('Major').agg({
    'GPA': ['mean', 'max', 'min'],
    'Age': 'mean'
}))

"""## üí° Exercise 3.2: Pandas Practice

Complete the following tasks:
"""

# Create this DataFrame first
products = pd.DataFrame({
    'Product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Accessories', 'Accessories'],
    'Price': [1200, 800, 500, 300, 100, 50],
    'Stock': [50, 150, 80, 60, 200, 300]
})

print("Products DataFrame:")
print(products)

# Task 1: Find products with price > 400
# Your code here:



# Task 2: Calculate total inventory value for each product (Price * Stock)
# Add it as a new column called 'Total_Value'
# Your code here:



# Task 3: Find the average price by category
# Your code here:



# Task 4: Sort products by Total_Value in descending order
# Your code here:

import pandas as pd

# Create this DataFrame first
products = pd.DataFrame({
    'Product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse'],
    'Category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Accessories', 'Accessories'],
    'Price': [1200, 800, 500, 300, 100, 50],
    'Stock': [50, 150, 80, 60, 200, 300]
})

print("Products DataFrame:")
print(products)

# Task 1: Find products with price > 400
# Your code here:
products_above_400 = products[products['Price'] > 400]
print("Task 1: Products with Price > 400")
print(products_above_400)


# Task 2: Calculate total inventory value for each product (Price * Stock)
# Add it as a new column called 'Total_Value'
# Your code here:
products['Total_Value'] = products['Price'] * products['Stock']
print("Task 2: Products with Total_Value")
print(products)


# Task 3: Find the average price by category
# Your code here:
average_price_by_category = products.groupby('Category')['Price'].mean()
print("Task 3: Average Price by Category")
print(average_price_by_category)


# Task 4: Sort products by Total_Value in descending order
# Your code here:
sorted_products = products.sort_values(by='Total_Value', ascending=False)
print("Task 4: Products sorted by Total_Value (descending)")
print(sorted_products)

"""---

# Part 4: Data Preprocessing Tutorial üîß

Data preprocessing is a critical step in the ML pipeline. We'll cover the essential techniques using a realistic dataset.
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Create a sample dataset with realistic issues
np.random.seed(42)

n_samples = 100

# Generate data with intentional issues for preprocessing practice
data = {
    'Student_ID': range(1001, 1001 + n_samples),
    'Name': [f'Student_{i}' for i in range(n_samples)],
    'Age': np.random.randint(18, 30, n_samples).astype(float),
    'Gender': np.random.choice(['Male', 'Female', 'M', 'F', 'male', 'female', None], n_samples),
    'GPA': np.round(np.random.uniform(2.0, 4.0, n_samples), 2),
    'Attendance': np.random.uniform(60, 100, n_samples),
    'Study_Hours': np.random.uniform(5, 40, n_samples),
    'Previous_Grade': np.random.choice(['A', 'B', 'C', 'D', 'F', None], n_samples),
    'Scholarship': np.random.choice([True, False, 'Yes', 'No', None], n_samples)
}

# Introduce missing values
df = pd.DataFrame(data)
df.loc[np.random.choice(n_samples, 10), 'Age'] = np.nan
df.loc[np.random.choice(n_samples, 15), 'GPA'] = np.nan
df.loc[np.random.choice(n_samples, 8), 'Attendance'] = np.nan
df.loc[np.random.choice(n_samples, 12), 'Study_Hours'] = np.nan

# Introduce duplicates
df = pd.concat([df, df.iloc[:5]], ignore_index=True)

# Introduce outliers
df.loc[np.random.choice(len(df), 3), 'Age'] = [150, -5, 200]
df.loc[np.random.choice(len(df), 3), 'Study_Hours'] = [100, 150, -10]

print("Raw Dataset Preview:")
print(df.head(10))
print(f"\nDataset Shape: {df.shape}")

"""## 4.1 Initial Data Exploration"""

# ============================================
# BASIC EXPLORATION
# ============================================

print("="*60)
print("DATA EXPLORATION REPORT")
print("="*60)

print("\nüìä Dataset Info:")
print(df.info())

print("\nüìà Statistical Summary:")
print(df.describe())

print("\nüîç Missing Values:")
missing = df.isnull().sum()
missing_pct = (missing / len(df) * 100).round(2)
missing_df = pd.DataFrame({
    'Missing Count': missing,
    'Missing %': missing_pct
})
print(missing_df[missing_df['Missing Count'] > 0])

print("\nüîÑ Duplicate Rows:", df.duplicated().sum())

print("\nüìù Unique Values in Categorical Columns:")
for col in ['Gender', 'Previous_Grade', 'Scholarship']:
    print(f"\n{col}: {df[col].unique()}")

"""## 4.2 Handling Duplicates"""

# ============================================
# REMOVING DUPLICATES
# ============================================

print(f"Before removing duplicates: {len(df)} rows")

# Find duplicates
duplicates = df[df.duplicated(keep=False)]
print(f"\nDuplicate rows found: {len(duplicates)}")
print("\nDuplicate rows:")
print(duplicates.head())

# Remove duplicates (keep first occurrence)
df_clean = df.drop_duplicates(keep='first')
print(f"\nAfter removing duplicates: {len(df_clean)} rows")

"""## 4.3 Handling Missing Values"""

# ============================================
# MISSING VALUE STRATEGIES
# ============================================

print("STRATEGIES FOR HANDLING MISSING VALUES:")
print("="*50)

# Strategy 1: Drop rows with missing values
print("\n1Ô∏è‚É£ Strategy: Drop rows with missing values")
df_dropped = df_clean.dropna()
print(f"   Rows remaining: {len(df_dropped)} (lost {len(df_clean) - len(df_dropped)} rows)")
print("   ‚ö†Ô∏è Warning: May lose too much data!")

# Strategy 2: Fill with specific values
print("\n2Ô∏è‚É£ Strategy: Fill numerical columns with mean/median")
df_filled = df_clean.copy()

# For numerical columns - fill with median (more robust to outliers)
numerical_cols = ['Age', 'GPA', 'Attendance', 'Study_Hours']
for col in numerical_cols:
    median_val = df_filled[col].median()
    df_filled[col].fillna(median_val, inplace=True)
    print(f"   {col}: filled with median = {median_val:.2f}")

# Strategy 3: Fill categorical with mode
print("\n3Ô∏è‚É£ Strategy: Fill categorical columns with mode")
categorical_cols = ['Gender', 'Previous_Grade', 'Scholarship']
for col in categorical_cols:
    mode_val = df_filled[col].mode()[0] if not df_filled[col].mode().empty else 'Unknown'
    df_filled[col].fillna(mode_val, inplace=True)
    print(f"   {col}: filled with mode = '{mode_val}'")

# Verify no missing values remain
print("\n‚úÖ Missing values after filling:")
print(df_filled.isnull().sum().sum(), "missing values remaining")

"""## 4.4 Handling Outliers"""

# ============================================
# DETECTING OUTLIERS
# ============================================

import matplotlib.pyplot as plt

def detect_outliers_iqr(data, column):
    """Detect outliers using IQR method"""
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

print("OUTLIER DETECTION (IQR Method):")
print("="*50)

for col in ['Age', 'Study_Hours']:
    outliers, lb, ub = detect_outliers_iqr(df_filled, col)
    print(f"\n{col}:")
    print(f"  Valid range: [{lb:.2f}, {ub:.2f}]")
    print(f"  Outliers found: {len(outliers)}")
    if len(outliers) > 0:
        print(f"  Outlier values: {outliers[col].values}")

# Visualize outliers with boxplots
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

for idx, col in enumerate(['Age', 'Study_Hours']):
    axes[idx].boxplot(df_filled[col].dropna())
    axes[idx].set_title(f'{col} - Before Handling Outliers')
    axes[idx].set_ylabel('Value')

plt.tight_layout()
plt.show()

# ============================================
# HANDLING OUTLIERS
# ============================================

print("\nHANDLING OUTLIERS:")
print("="*50)

df_no_outliers = df_filled.copy()

def cap_outliers(data, column):
    """Cap outliers to the boundary values"""
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    data[column] = data[column].clip(lower=lower_bound, upper=upper_bound)
    return data, lower_bound, upper_bound

# Cap outliers in Age and Study_Hours
for col in ['Age', 'Study_Hours']:
    df_no_outliers, lb, ub = cap_outliers(df_no_outliers, col)
    print(f"{col}: capped to [{lb:.2f}, {ub:.2f}]")

# Visualize after handling outliers
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

for idx, col in enumerate(['Age', 'Study_Hours']):
    axes[idx].boxplot(df_no_outliers[col].dropna())
    axes[idx].set_title(f'{col} - After Handling Outliers')
    axes[idx].set_ylabel('Value')

plt.tight_layout()
plt.show()

"""## 4.5 Data Standardization & Cleaning"""

# ============================================
# STANDARDIZING CATEGORICAL VALUES
# ============================================

print("STANDARDIZING CATEGORICAL VALUES:")
print("="*50)

df_standardized = df_no_outliers.copy()

# Standardize Gender column
print(f"\nGender - Before: {df_standardized['Gender'].unique()}")

gender_mapping = {
    'Male': 'Male', 'M': 'Male', 'male': 'Male',
    'Female': 'Female', 'F': 'Female', 'female': 'Female'
}
df_standardized['Gender'] = df_standardized['Gender'].replace(gender_mapping)

print(f"Gender - After: {df_standardized['Gender'].unique()}")

# Standardize Scholarship column
print(f"\nScholarship - Before: {df_standardized['Scholarship'].unique()}")

scholarship_mapping = {
    True: 'Yes', 'Yes': 'Yes',
    False: 'No', 'No': 'No'
}
df_standardized['Scholarship'] = df_standardized['Scholarship'].replace(scholarship_mapping)

print(f"Scholarship - After: {df_standardized['Scholarship'].unique()}")

"""## 4.6 Encoding Categorical Variables"""

# ============================================
# ENCODING CATEGORICAL VARIABLES
# ============================================

print("ENCODING CATEGORICAL VARIABLES:")
print("="*50)

df_encoded = df_standardized.copy()

# Label Encoding for binary categorical variables
print("\n1Ô∏è‚É£ Label Encoding (for binary/ordinal variables):")

# Gender: Binary encoding
df_encoded['Gender_Encoded'] = df_encoded['Gender'].map({'Male': 0, 'Female': 1})
print(f"   Gender: Male=0, Female=1")

# Previous Grade: Ordinal encoding
grade_order = {'A': 4, 'B': 3, 'C': 2, 'D': 1, 'F': 0}
df_encoded['Grade_Encoded'] = df_encoded['Previous_Grade'].map(grade_order)
print(f"   Previous_Grade: F=0, D=1, C=2, B=3, A=4")

# Scholarship: Binary encoding
df_encoded['Scholarship_Encoded'] = df_encoded['Scholarship'].map({'No': 0, 'Yes': 1})
print(f"   Scholarship: No=0, Yes=1")

# One-Hot Encoding (for nominal variables with no order)
print("\n2Ô∏è‚É£ One-Hot Encoding (for nominal variables):")

# One-hot encode Gender (demonstration)
gender_dummies = pd.get_dummies(df_encoded['Gender'], prefix='Gender')
print("   Gender one-hot encoded:")
print(gender_dummies.head())

print("\nüìä Encoded DataFrame Preview:")
print(df_encoded[['Name', 'Gender', 'Gender_Encoded', 'Previous_Grade',
                  'Grade_Encoded', 'Scholarship', 'Scholarship_Encoded']].head(10))

"""## 4.7 Feature Scaling"""

# ============================================
# FEATURE SCALING
# ============================================

print("FEATURE SCALING:")
print("="*50)

numerical_features = ['Age', 'GPA', 'Attendance', 'Study_Hours']

print("\nOriginal values (before scaling):")
print(df_encoded[numerical_features].describe().round(2))

# Min-Max Scaling (Normalization) - scales to [0, 1]
print("\n1Ô∏è‚É£ Min-Max Scaling (Normalization) - scales to [0, 1]:")

df_scaled = df_encoded.copy()

for col in numerical_features:
    min_val = df_scaled[col].min()
    max_val = df_scaled[col].max()
    df_scaled[f'{col}_MinMax'] = (df_scaled[col] - min_val) / (max_val - min_val)

print(df_scaled[[f'{col}_MinMax' for col in numerical_features]].describe().round(2))

# Standard Scaling (Z-score normalization) - mean=0, std=1
print("\n2Ô∏è‚É£ Standard Scaling (Z-score) - mean=0, std=1:")

for col in numerical_features:
    mean_val = df_scaled[col].mean()
    std_val = df_scaled[col].std()
    df_scaled[f'{col}_ZScore'] = (df_scaled[col] - mean_val) / std_val

print(df_scaled[[f'{col}_ZScore' for col in numerical_features]].describe().round(2))

"""## 4.8 Final Preprocessed Dataset"""

# ============================================
# PREPARE FINAL DATASET FOR ML
# ============================================

# Select relevant columns for ML model
ml_features = ['Age', 'GPA', 'Attendance', 'Study_Hours',
               'Gender_Encoded', 'Grade_Encoded', 'Scholarship_Encoded']

df_final = df_scaled[['Student_ID', 'Name'] + ml_features].copy()

print("="*60)
print("FINAL PREPROCESSED DATASET FOR ML")
print("="*60)
print(f"\nShape: {df_final.shape}")
print(f"\nColumns: {list(df_final.columns)}")
print("\nPreview:")
print(df_final.head(10))
print("\nData Types:")
print(df_final.dtypes)
print("\n‚úÖ Dataset is ready for Machine Learning!")

"""## üí° Exercise 4.1: Comprehensive Preprocessing Challenge

Apply all preprocessing techniques to the following dataset:
"""

# Challenge Dataset
np.random.seed(123)

challenge_data = {
    'Employee_ID': range(1, 51),
    'Department': np.random.choice(['IT', 'HR', 'Sales', 'it', 'hr', 'SALES', None], 50),
    'Salary': np.random.uniform(30000, 150000, 50),
    'Experience_Years': np.random.uniform(0, 20, 50),
    'Performance_Rating': np.random.choice(['Excellent', 'Good', 'Average', 'Poor', None], 50),
    'Remote_Work': np.random.choice([True, False, 'Yes', 'No', '1', '0', None], 50)
}

challenge_df = pd.DataFrame(challenge_data)

# Add some missing values
challenge_df.loc[np.random.choice(50, 7), 'Salary'] = np.nan
challenge_df.loc[np.random.choice(50, 5), 'Experience_Years'] = np.nan

# Add some outliers
challenge_df.loc[np.random.choice(50, 2), 'Salary'] = [500000, -10000]
challenge_df.loc[np.random.choice(50, 2), 'Experience_Years'] = [50, -5]

# Add duplicates
challenge_df = pd.concat([challenge_df, challenge_df.iloc[:3]], ignore_index=True)

print("Challenge Dataset:")
print(challenge_df.head(15))
print(f"\nShape: {challenge_df.shape}")

# YOUR PREPROCESSING CODE HERE
# Tasks:
# 1. Explore the data (missing values, duplicates, unique values)
# 2. Remove duplicates
# 3. Handle missing values
# 4. Handle outliers in Salary and Experience_Years
# 5. Standardize Department and Remote_Work columns
# 6. Encode categorical variables
# 7. Scale numerical features

# Step 1: Explore the data



# Step 2: Remove duplicates



# Step 3: Handle missing values



# Step 4: Handle outliers



# Step 5: Standardize categorical values



# Step 6: Encode categorical variables



# Step 7: Scale numerical features

"""## Explore Data

Perform initial data exploration to identify missing values, duplicates, and unique values in categorical columns of the challenge_df.

"""

import numpy as np
import pandas as pd

np.random.seed(123)

challenge_data = {
    'Employee_ID': range(1, 51),
    'Department': np.random.choice(['IT', 'HR', 'Sales', 'it', 'hr', 'SALES', None], 50),
    'Salary': np.random.uniform(30000, 150000, 50),
    'Experience_Years': np.random.uniform(0, 20, 50),
    'Performance_Rating': np.random.choice(['Excellent', 'Good', 'Average', 'Poor', None], 50),
    'Remote_Work': np.random.choice([True, False, 'Yes', 'No', '1', '0', None], 50)
}

challenge_df = pd.DataFrame(challenge_data)

challenge_df.loc[np.random.choice(50, 7), 'Salary'] = np.nan
challenge_df.loc[np.random.choice(50, 5), 'Experience_Years'] = np.nan

challenge_df.loc[np.random.choice(50, 2), 'Salary'] = [500000, -10000]
challenge_df.loc[np.random.choice(50, 2), 'Experience_Years'] = [50, -5]

challenge_df = pd.concat([challenge_df, challenge_df.iloc[:3]], ignore_index=True)

challenge_df_processed = challenge_df.copy()

print("="*60)
print("CHALLENGE DATA EXPLORATION REPORT")
print("="*60)

print(f"Initial shape of challenge_df_processed: {challenge_df_processed.shape}")

print(" Dataset Info:")
print(challenge_df_processed.info())

print(" Statistical Summary:")
print(challenge_df_processed.describe())

print(" Missing Values:")
missing = challenge_df_processed.isnull().sum()
missing_pct = (missing / len(challenge_df_processed) * 100).round(2)
missing_df = pd.DataFrame({
    'Missing Count': missing,
    'Missing %': missing_pct
})
print(missing_df[missing_df['Missing Count'] > 0])

print(" Duplicate Rows:", challenge_df_processed.duplicated().sum())

print(" Unique Values in Categorical Columns:")
for col in ['Department', 'Performance_Rating', 'Remote_Work']:
    print(f"{col}: {challenge_df_processed[col].unique()}")

"""## Remove Duplicates

this code remove the duplicate rows. This involves printing the initial row count, using `drop_duplicates` to remove them, and then printing the row count again to show the effect of the operation.
"""

print(f"Before removing duplicates: {len(challenge_df_processed)} rows")

challenge_df_processed.drop_duplicates(inplace=True)

print(f"After removing duplicates: {len(challenge_df_processed)} rows")

"""## Handle Missing Values

Fill missing values in numerical columns with the median and in categorical columns with the mode.missing values can significantly impact model performance. For numerical features like 'Salary' and 'Experience_Years', imputing with the median is generally a robust approach as it is less sensitive to outliers compared to the mean. For categorical features ('Department', 'Performance_Rating', 'Remote_Work'), imputing with the mode (most frequent value) helps preserve the distribution of the categories.
"""

print("Handling Missing Values...")

numerical_cols = ['Salary', 'Experience_Years']
categorical_cols = ['Department', 'Performance_Rating', 'Remote_Work']

for col in numerical_cols:
    median_val = challenge_df_processed[col].median()
    challenge_df_processed[col] = challenge_df_processed[col].fillna(median_val)
    print(f"  Filled missing values in '{col}' with median: {median_val:.2f}")

for col in categorical_cols:
    mode_val = challenge_df_processed[col].mode()[0] if not challenge_df_processed[col].mode().empty else 'Unknown'
    challenge_df_processed[col] = challenge_df_processed[col].fillna(mode_val)
    print(f"  Filled missing values in '{col}' with mode: '{mode_val}'")

print("Verification of missing values after imputation:")
missing_after_imputation = challenge_df_processed[numerical_cols + categorical_cols].isnull().sum()
print(missing_after_imputation[missing_after_imputation > 0])

if missing_after_imputation.sum() == 0:
    print(" All specified columns now have no missing values.")
else:
    print(" Some missing values still remain in specified columns.")

"""## Handle Outliers

Outliers can disproportionately influence statistical analyses and machine learning models. For 'Salary' and 'Experience_Years', I will use the Interquartile Range (IQR) method to detect outliers. This method is robust to skewed data and defines outliers as values falling outside the range `[Q1 - 1.5*IQR, Q3 + 1.5*IQR]`. Instead of removing these outliers, which could lead to data loss, I will cap them, meaning values below the lower bound will be set to the lower bound, and values above the upper bound will be set to the upper bound. This approach helps to retain data while mitigating the extreme effects of outliers.
"""

print("Detecting and Handling Outliers (IQR Method):")
print("="*60)

def cap_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers_before = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    if not outliers_before.empty:
        print(f"Outliers in '{column}' before capping:")
        print(f"  Valid range: [{lower_bound:.2f}, {upper_bound:.2f}]")
        print(f"  Number of outliers: {len(outliers_before)}")
        print(f"  Outlier values: {outliers_before[column].values}")

    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
    print(f"  '{column}' outliers capped to range [{lower_bound:.2f}, {upper_bound:.2f}]")
    return df

for col in ['Salary', 'Experience_Years']:
    challenge_df_processed = cap_outliers_iqr(challenge_df_processed, col)

print("Verification of numerical columns after outlier handling:")
print(challenge_df_processed[['Salary', 'Experience_Years']].describe().round(2))

"""## Standardize Categorical Values

Clean and standardize inconsistent entries in 'Department', 'Remote_Work', and 'Performance_Rating' columns.
categorical data often comes with inconsistencies, such as variations in casing ('IT', 'it') or different representations for the same concept (True, 'Yes', '1' for 'Remote_Work'). Standardizing these entries is crucial for accurate analysis and encoding, ensuring that the model treats similar categories uniformly.

I will apply these standardization:
1.  **Department**: Convert all entries to a consistent format (e.g., title case) to group 'IT', 'it', 'HR', 'hr', 'Sales', 'SALES' into unified categories.
2.  **Remote_Work**: Map all positive indicators (True, 'Yes', '1') to 'Yes' and all negative indicators (False, 'No', '0') to 'No'.
3.  **Performance_Rating**: Although missing values were handled by filling with mode, (confirm its unique values remain consistent).
"""

print("Standardizing Categorical Values...")

print(f"'Department' - Before standardization: {challenge_df_processed['Department'].unique()}")
challenge_df_processed['Department'] = challenge_df_processed['Department'].astype(str).str.capitalize()
print(f"'Department' - After standardization: {challenge_df_processed['Department'].unique()}")

print(f"'Remote_Work' - Before standardization: {challenge_df_processed['Remote_Work'].unique()}")
remote_work_mapping = {
    True: 'Yes', 'Yes': 'Yes', '1': 'Yes',
    False: 'No', 'No': 'No', '0': 'No'
}
challenge_df_processed['Remote_Work'] = challenge_df_processed['Remote_Work'].replace(remote_work_mapping)
print(f"'Remote_Work' - After standardization: {challenge_df_processed['Remote_Work'].unique()}")

print("Verification of unique values after standardization:")
for col in ['Department', 'Performance_Rating', 'Remote_Work']:
    print(f"  {col}: {challenge_df_processed[col].unique()}")

"""## Encode Categorical Variables

Encode 'Remote_Work' and 'Performance_Rating' using label/ordinal encoding and 'Department' using one-hot encoding.encoding categorical variables is essential for machine learning models, as most algorithms require numerical input. I will use two main encoding strategies:

1.  **Label/Ordinal Encoding**: This is suitable for binary categorical variables or ordinal variables where there's a natural order.
    -   `Remote_Work`: This is a binary variable ('Yes', 'No'), so label encoding (0 or 1) is appropriate.
    -   `Performance_Rating`: This is an ordinal variable ('Poor', 'Average', 'Good', 'Excellent') with a clear hierarchy, so ordinal encoding with assigned numerical values will preserve this order.

2.  **One-Hot Encoding**: This is best for nominal categorical variables where there is no inherent order. Using one-hot encoding prevents the model from assuming any artificial ordinal relationship.
    -   `Department`: This column ('Hr', 'Sales', 'It') has no intrinsic order, making one-hot encoding the suitable choice.
"""

print("Encoding Categorical Variables...")

df_encoded_challenge = challenge_df_processed.copy()

print(" Label Encoding (for binary/ordinal variables):")

df_encoded_challenge['Remote_Work_Encoded'] = df_encoded_challenge['Remote_Work'].map({'No': 0, 'Yes': 1})
print(f"   Remote_Work: No=0, Yes=1")

performance_rating_order = {'Poor': 0, 'Average': 1, 'Good': 2, 'Excellent': 3}
df_encoded_challenge['Performance_Rating_Encoded'] = df_encoded_challenge['Performance_Rating'].map(performance_rating_order)
print(f"   Performance_Rating: Poor=0, Average=1, Good=2, Excellent=3")

print(" One-Hot Encoding (for nominal variables):")

department_dummies = pd.get_dummies(df_encoded_challenge['Department'], prefix='Department', dtype=int)
df_encoded_challenge = pd.concat([df_encoded_challenge, department_dummies], axis=1)
print("   Department one-hot encoded and added to DataFrame.")

print(" Encoded DataFrame Preview (relevant columns):")
print(df_encoded_challenge[['Department', 'Department_Hr', 'Department_It', 'Department_Sales',
                            'Remote_Work', 'Remote_Work_Encoded',
                            'Performance_Rating', 'Performance_Rating_Encoded']].head())

"""## Feature Scaling

Apply standard scaling Z-score normalization(This method transforms data to have a mean of 0 and a standard deviation of 1. ) to 'Salary' and 'Experience_Years'. feature scaling is a crucial preprocessing step, especially for algorithms that are sensitive to the magnitude of numerical features Without scaling, features with larger values might dominate the learning process.
"""

print("Applying Feature Scaling (Standard Scaling)...")

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

numerical_features_to_scale = ['Salary', 'Experience_Years']

df_scaled_challenge = df_encoded_challenge.copy()

df_scaled_challenge[numerical_features_to_scale] = scaler.fit_transform(df_scaled_challenge[numerical_features_to_scale])

print("Scaled Numerical Features Preview (first 5 rows):")
print(df_scaled_challenge[numerical_features_to_scale].head())

print("Descriptive Statistics of Scaled Numerical Features:")
print(df_scaled_challenge[numerical_features_to_scale].describe().round(2))

"""---

# üìù Lab Summary

## Key Takeaways:

### 1. ML Project Lifecycle
- 7 stages: Problem Definition ‚Üí Data Collection ‚Üí Data Preparation ‚Üí Feature Engineering ‚Üí Model Training ‚Üí Model Evaluation ‚Üí Deployment
- Data preparation takes 60-80% of the time

### 2. Environment Setup
- **Google Colab**: Cloud-based, no setup, free GPU
- **VSCode**: Local, customizable, requires setup

### 3. NumPy Essentials
- Array creation: `np.array()`, `np.zeros()`, `np.ones()`
- Operations: element-wise, statistical functions
- Indexing and slicing: similar to Python lists

### 4. Pandas Essentials
- Series (1D) and DataFrame (2D)
- Selection: `[]`, `.loc[]`, `.iloc[]`
- Filtering, grouping, aggregation

### 5. Data Preprocessing Steps
1. **Explore** data (info, describe, missing values)
2. **Remove duplicates** (drop_duplicates)
3. **Handle missing values** (drop, fill with mean/median/mode)
4. **Handle outliers** (IQR method, capping)
5. **Standardize** categorical values
6. **Encode** categorical variables (label, one-hot)
7. **Scale** numerical features (min-max, z-score)

---

# üìö Lab Assignment

## Instructions:

1. Complete all exercises in this notebook (Exercises 1.1, 3.1, 3.2, and 4.1)
2. For Exercise 4.1, document each step with comments explaining your choices
3. Save your completed notebook with your name: `Lab1_YourName.ipynb`
4. Create a GitHub repository dedicated to this course (e.g., CS4082-Machine-Learning-Labs).

*   This repository will be used to store all lab work for the entire semester.

*   Organize your work clearly (e.g., one folder per lab).
5. Upload Lab 1 (Lab1_YourName.ipynb) to your GitHub repository in an appropriate folder (e.g., Lab1/).
6. Submit the GitHub repository link via Blackboard before the deadline.

## Grading Rubric:

| Component | Points |
|-----------|--------|
| Exercise 1.1 (Problem Definition) | 10 |
| Exercise 3.1 (NumPy) | 15 |
| Exercise 3.2 (Pandas) | 15 |
| Exercise 4.1 (Preprocessing Challenge) | 50 |
| Code Quality & Comments | 10 |
| **Total** | **100** |

---

**Good luck! üçÄ**
"""